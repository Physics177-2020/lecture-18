{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvZNupW5M1ys"
   },
   "source": [
    "# Lecture 18: Loss functions and regression\n",
    "\n",
    "Many problems in optimization and statistical inference can be fomulated as the minimization of a [loss function](https://en.wikipedia.org/wiki/Loss_function). One example of this that we will explore today is [linear regression](https://en.wikipedia.org/wiki/Linear_regression). \n",
    "\n",
    "In the linear regression problem, we assume that there is a linear relationship between a measured *response variable* $y$ and one (or more) *independent variable(s)* $x$. If this relationship was exact, then we would only need a single measurement to determine the coefficient relating $x$ and $y$. However, there may be some experimental error or noise associated with measuring these variables, or the relationship between $x$ and $y$ may be probabilistic instead of deterministic. In such cases, we need to make multiple measurements and analyze them carefully to determine the relationship.\n",
    "\n",
    "Let's imagine that we test a whole range of values for the independent variable $X = \\{x_1, x_2, \\ldots, x_N\\}$, and for each one we measure the value of the response variable $Y = \\{y_1, y_2, \\ldots, y_N\\}$. One simple way to determine the linear relationship between the $x$ and $y$ variables is to search for a parameter $\\alpha$ that minimizes the squared difference between the predicted and measured values of $y$,\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\sum_{i=1}^{N}\\left(y_i - \\alpha x_i\\right)^2\\,.\n",
    "$$\n",
    "\n",
    "Here, $L(\\alpha)$ is our loss function. This \"version\" of linear regression is called [ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares), or OLS. Sometimes for convenience we can express the above equation in vector form,\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\left(Y - \\alpha X\\right)^2\\,.\n",
    "$$\n",
    "\n",
    "In this simple case, we can analytically find the value of $\\alpha$ that minimizes the loss function. This is\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = \\left(X^T X\\right)^{-1}X^T Y\\,.\n",
    "$$\n",
    "\n",
    "Here the hat $\\hat{\\cdot}$ over the $\\alpha$ variable denotes our *estimate* of the \"true\" value of $\\alpha$, based on our data. \n",
    "\n",
    "\n",
    "### Example: Estimating a noisy linear relationship\n",
    "\n",
    "Let's assume that we are trying to measure the spring constant of a spring by stretching it to different distances and measuring the force of resistance. We'll assume that the measurements aren't completely precise, due to the bouncing of the spring and other factors. Mathematically we can write this as\n",
    "\n",
    "$$\n",
    "F = -k x + \\epsilon\\,,\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a random noise term. We can assume that $\\epsilon$ follows a Gaussian distribution,\n",
    "\n",
    "$$\n",
    "P(\\epsilon) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{\\epsilon^2}{2\\sigma^2}}\\,.\n",
    "$$\n",
    "\n",
    "Let's begin by first generating a simulated data set using these assumptions. We'll assume that $k = 1$ and $\\sigma = 0.4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "WESNiRsEM1yt",
    "outputId": "299344dd-ca92-4dcd-c91b-9f272d6a3cea"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define spring parameters and choose a range of x values to test\n",
    "\n",
    "k     = 1.0\n",
    "sigma = 0.4\n",
    "\n",
    "X = np.arange(0.2, 4.2, 0.2)\n",
    "Y = -k*X + rng.normal(loc=0, scale=sigma, size=len(X))\n",
    "\n",
    "\n",
    "# Plot the measurements\n",
    "\n",
    "sns.scatterplot(X, Y)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$F$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwrhSycWM1yw"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jj--50KQM1yz"
   },
   "source": [
    "### Estimating the relationship\n",
    "\n",
    "Now we can apply our formula to estimate the relationship between the force and the stretch, the spring constant $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "gVNZOrY5M1y0",
    "outputId": "9920099c-9970-4f5b-893a-f6d7dc5c4527"
   },
   "outputs": [],
   "source": [
    "# Estimate the spring constant with OLS\n",
    "\n",
    "k_hat = -np.sum(X*Y)/np.sum(X*X)\n",
    "\n",
    "print('The estimated spring constant is {}, true value is {}'.format(k_hat, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGBEU7v8M1y3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPI4h6vGM1y6"
   },
   "source": [
    "We can also use some standard Python packages to do this. Let's try this using the `statsmodels` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 541
    },
    "colab_type": "code",
    "id": "lERitbWsM1y7",
    "outputId": "f68bd8dd-1fa2-48ff-b719-cdbe91bcad18"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model   = sm.OLS(Y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frJWTXqrM1y9"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "th5En8gYM1zA"
   },
   "source": [
    "The output is much more verbose (giving lots of statistical information), but we find the same result.\n",
    "\n",
    "### The effects of finite sampling\n",
    "\n",
    "Now, let's explore what happens when we change the number of measurements or the strength of the noise. \n",
    "\n",
    "**Exercise:** Copy your code from the first code block above and experiment with different numbers of data points (i.e., the size of the `X` vector) and different levels of noise (measured by the standard deviation `sigma`). How does the robustness of the results change as you increase the noise or decrease the number of samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ffClai9M1zB"
   },
   "outputs": [],
   "source": [
    "# < Your code here! >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YxXiftxpM1zE"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d34icHWM1zH"
   },
   "source": [
    "### Regularization\n",
    "\n",
    "We noticed that, when the noise is strong and when the number of samples is very small, it becomes more difficult to estimate the relationship between $x$ and $y$. In the extreme, when we have just a few samples, some estimates can even be of the wrong sign.\n",
    "\n",
    "One way to control our estimates is to use [regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)). Here, we'll implement a simple strategy sometimes referred to as [Tikhonov](https://en.wikipedia.org/wiki/Tikhonov_regularization) or $L_2$-norm regularization. Returning to our loss function, we can also penalize large values of the inferred linear coefficient:\n",
    "\n",
    "$$\n",
    "L(\\alpha) = \\left(Y - \\alpha X\\right)^2 + \\gamma\\alpha^2\\,.\n",
    "$$\n",
    "\n",
    "Again, in this case we can analytically find the estimate of $\\alpha$ that minimizes the loss function,\n",
    "\n",
    "$$\n",
    "\\hat{\\alpha} = \\left(X^T X + \\gamma I\\right)^{-1}X^T Y\\,.\n",
    "$$\n",
    "\n",
    "Let's test this version of regression below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "De5hK_4_M1zI"
   },
   "outputs": [],
   "source": [
    "# Define spring parameters and choose a range of x values to test\n",
    "\n",
    "gamma = 0.5\n",
    "k     = 1.0\n",
    "sigma = 1.0\n",
    "\n",
    "X = np.arange(0.2, 2.2, 0.6)\n",
    "Y = -k*X + rng.normal(loc=0, scale=sigma, size=len(X))\n",
    "\n",
    "\n",
    "# Get the estimated spring constant with and without regularization\n",
    "\n",
    "k_hat     = -np.sum(X*Y)/np.sum(X*X)\n",
    "k_hat_reg = -np.sum(X*Y)/(np.sum(X*X) + gamma)\n",
    "\n",
    "print('The estimated spring constant is {}'.format(k_hat))\n",
    "print('The estimated spring constant is {} (regularized)'.format(k_hat_reg))\n",
    "\n",
    "\n",
    "# Plot the measurements\n",
    "\n",
    "sns.scatterplot(X, Y)\n",
    "sns.lineplot(X, -k_hat*X, label='OLS')\n",
    "sns.lineplot(X, -k_hat_reg*X, label='Reg')\n",
    "sns.lineplot(X, -k*X, label='True')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$F$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1IvAVByM1zL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w3EjR2CTM1zO"
   },
   "source": [
    "### Checking the variance of the estimate\n",
    "\n",
    "One advantage of regularization is that it reduced the variance of the parameters that we estimate. Let's run many measurement simulations below and record the estimates of $k$ that we get in each case, then compare them with and without regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xrs3L6JRM1zP"
   },
   "outputs": [],
   "source": [
    "# Define spring parameters and choose a range of x values to test\n",
    "\n",
    "gamma = 0.5\n",
    "k     = 1.0\n",
    "sigma = 1.0\n",
    "\n",
    "k_hats     = []\n",
    "k_hat_regs = []\n",
    "\n",
    "for i in range(10000):\n",
    "    X = np.arange(0.2, 2.2, 0.8)\n",
    "    Y = -k*X + rng.normal(loc=0, scale=sigma, size=len(X))\n",
    "\n",
    "    k_hats.append(-np.sum(X*Y)/np.sum(X*X))\n",
    "    k_hat_regs.append(-np.sum(X*Y)/(np.sum(X*X) + gamma))\n",
    "\n",
    "print('Estimate of k mean: %.2f, std: %.2f' % (np.mean(k_hats), np.std(k_hats)))\n",
    "print('Estimate of k mean: %.2f, std: %.2f\\t(regularized)' % (np.mean(k_hat_regs), np.std(k_hat_regs)))\n",
    "\n",
    "# Plot a histogram of the results\n",
    "\n",
    "sns.distplot(k_hats, norm_hist=True, kde=False, label='OLS')\n",
    "sns.distplot(k_hat_regs, norm_hist=True, kde=False, label='Reg')\n",
    "plt.title(r'Distribution of $\\hat{k}$ over $10^4$ random samples')\n",
    "plt.xlabel(r'$\\hat{k}$')\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GFzBeFPM1zX"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JoqzB1RIM1za"
   },
   "source": [
    "Our estimates show a [tradeoff](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff) between the bias of the estimate and its variance -- the regularized estimate tends to be smaller than the true value of $k$, but it has lower variance than the unregularized estimate. **Typically**, it is a good idea in inference to introduce some bias in order to reduce the variance, but such questions must always be approached case by case."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of lecture-17.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
